What is the difference between simple linear regression and multiple linear regression?
Linear Regression
Also called simple regression, linear regression establishes the relationship between two variables. Linear regression is graphically depicted using a straight line with the slope defining how the change in one variable impacts a change in the other. The y-intercept of a linear regression relationship represents the value of one variable when the value of the other is 0.

Multiple Regression
For complex connections between data, the relationship might be explained by more than one variable. In this case, an analyst uses multiple regression which attempts to explain a dependent variable using more than one independent variable.

Cost Function?
A cost function is an important parameter that determines how well a machine learning model performs for a given dataset. It calculates the difference between the expected value and predicted value and represents it as a single real number.

 "Cost function is a measure of how wrong the model is in estimating the relationship between X(input) and Y(output) Parameter."

 

do you interpret the coefficients in a linear regression model?

In a linear regression model, the size of the coefficient for each independent variable gives you the size of the effect that variable is having on your dependent variable
The sign on the coefficient (positive or negative) gives you the direction of the effect. A positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent variable also tends to increase.

What are the assumptions of linear regression?
The assumptions of linear regression are:
Linear relationship between the predictor(s) and the dependent variable
Multivariate normality of the residuals
No or little multicollinearity among the predictors
No auto-correlation of the residuals
Homoscedasticity or equal variance of the residuals



Linear Regression is one of the most simple Machine learning algorithm that comes under Supervised Learning technique and used for solving regression problems.
It is used for predicting the continuous dependent variable with the help of independent variables.
The goal of the Linear regression is to find the best fit line that can accurately predict the output for the continuous dependent variable.
If single independent variable is used for prediction then it is called Simple Linear Regression and if there are more than two independent variables then such regression is called as Multiple Linear Regression

----------------------------------------------------------------------------------------------------------------------------------------------

How does logistic regression differ from linear regression?
Linear Regression and Logistic Regression are the two famous Machine Learning Algorithms which come under supervised learning technique. Since both the algorithms are of supervised in nature hence these algorithms use labeled dataset to make the predictions. But the main difference between them is how they are being used. The Linear Regression is used for solving Regression problems whereas Logistic Regression is used for solving the Classification problems.






Explain the sigmoid function and its role in logistic regression?
The sigmoid function is used in logistic regression to map predicted values to probabilities1234. It maps any real value into another value within a range of 0 and 11. The sigmoid function forms an S shaped graph, which means as x approaches infinity, the probability becomes 1, and as x approaches negative infinity,the probality become 0


the key performance metrics used to evaluate a logistic regression model?
There are few metrics using which we can evaluate a logistic regression model, 1) AIC (Akaike Information Criteria) 2) Confusion matrix 3) ROC curve 4) Null deviance and residual deviance



How do you handle multicollinearity in logistic regression?
To avoid multicollinearity in logistic regression, you can Remove some of the highly correlated independent variables.Linearly combine the independent variables, such as adding them together.Use partial least squares regression to create a set of uncorrelated components to include in the model.Use LASSO and Ridge regression, which are advanced forms of regression analysis that can handle multicollinearity.Use regularization, which adds an additional constraint on the weight vector and can handle multicollinearity.

----------------------------------------------------------------------------------------------------------------------------------------


What is the Naive Bayes algorithm based on?
Naive Bayes is a machine learning algorithm that predicts the class of data based on probabilities

Explain the concept of conditional probability in the context of Naive Bayes?
Conditional Probability: When we try to calculate probability on a condition, i.e. probability of happening of event A when event B has already taken place

What are the advantages and disadvantages of Naive Bayes?
advantage
1.Simple to implement
2.Handles missing data well
3.Fast and scalable
4.Simple to understand
5.Performs well in text classification
6.Works well with small datasets

disadvantage
1.Assumption of independence
2.Lack of flexibility
3.Data scarcity
4.Sensitivity to outliers
5.Class imbalance
6.Limited ability to capture interactions between features

How does Naive Bayes handle missing values and categorical features?
Naive Bayes can handle missing data. Attributes are handled separately by the algorithm at both model construction time and prediction time.
As such, if a data instance has a missing value for an attribute, it can be ignored while preparing the model, and ignored when a probability is calculated for a class value.


---------------------------------------------------------------------------------------------------------------------------------------------

How does a decision tree make decisions?

The decision tree operates by analyzing the data set to predict its classification.
 It commences from the tree’s root node, where the algorithm views the value of the root attribute compared to the attribute of the record in the actual data set. 


What are the main criteria for splitting nodes in a decision tree?
The Simple Math behind 3 Decision Tree Splitting criterions

1. Gini Impurity According to Wikipedia, Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. ...
2. Entropy Another very popular way to split nodes in the decision tree is Entropy. Entropy is the measure of Randomness in the system. ...
3. Variance


How do decision trees handle categorical variables?
Decision trees handle categorical variables naturally. Every split in a decision tree is based on a feature. If the feature is categorical, the split is done with the elements belonging to a particular class. 


What are some common techniques to prevent overfitting in decision trees?
To prevent overfitting in decision trees, you can
Identify when overfitting in decision trees
Prevent overfitting with early stopping
Limit tree depth
Do not consider splits that do not reduce classification error
Do not split intermediate nodes with only few points
Prune complex trees
Use a total cost formula that balances classification error and tree complexity
Set a maximum depth
Set a minimum number of examples in leaf


-----------------------------------------------------------------------------------------------------------------------------------


What is the basic idea behind SVM?
Support Vector Machines (SVMs) are a type of supervised learning algorithm that can be used for classification or regression tasks1. The main idea behind SVMs is to find a hyperplane that maximally separates the different classes in the training data1.


Explain the concepts of margin and support vectors in SVM?
Support Vector Machine 
Hyperplane: Hyperplane is the decision boundary that is used to separate the data points of different classes in a feature space. In the case of linear classifications, it will be a linear equation i.e. wx+b = 0.

Support Vectors: Support vectors are the closest data points to the hyperplane, which makes a critical role in deciding the hyperplane and margin. 

Margin: Margin is the distance between the support vector and hyperplane. The main objective of the support vector machine algorithm is to maximize the margin.  The wider margin indicates better classification performance.

Kernel: Kernel is the mathematical function, which is used in SVM to map the original input data points into high-dimensional feature spaces, so, that the hyperplane can be easily found out even if the data points are not linearly separable in the original input space. Some of the common kernel functions are linear, polynomial, radial basis function(RBF), and sigmoid.

Hard Margin: The maximum-margin hyperplane or the hard margin hyperplane is a hyperplane that properly separates the data points of different categories without any misclassifications.

Soft Margin: When the data is not perfectly separable or contains outliers, SVM permits a soft margin technique. Each data point has a slack variable introduced by the soft-margin SVM formulation, which softens the strict margin requirement and permits certain misclassifications or violations. It discovers a compromise between increasing the margin and reducing violations.

C: Margin maximisation and misclassification fines are balanced by the regularisation parameter C in SVM. The penalty for going over the margin or misclassifying data items is decided by it. A stricter penalty is imposed with a greater value of C, which results in a smaller margin and perhaps fewer misclassifications.


What are the different kernel functions used in SVM, and when would you use each?
Kernel Function is a method used to take data as input and transform it into the required form of processing data. “Kernel” is used due to a set of mathematical functions used in Support Vector Machine providing the window to manipulate the data.



How does SVM handle outliers?
Soft Margin: When the data is not perfectly separable or contains outliers, SVM permits a soft margin technique. Each data point has a slack variable introduced by the soft-margin SVM formulation, which softens the strict margin requirement and permits certain misclassifications or violations
